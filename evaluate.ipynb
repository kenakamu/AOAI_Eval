{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv \n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize Azure AI project and Azure OpenAI conncetion with your environment variables\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"AZURE_PROJECT_CONNECTION_STRING\"),\n",
    ")\n",
    "\n",
    "# Define ml_client to register custom evaluator\n",
    "ml_client = MLClient(\n",
    "       subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "       resource_group_name=os.environ[\"AZURE_RESOURCE_GROUP\"],\n",
    "       workspace_name=os.environ[\"AZURE_PROJECT_NAME\"],\n",
    "       credential=DefaultAzureCredential()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and quality evaluator\n",
    "- GroundednessEvaluator uses LLM \n",
    "- GroundednessProEvaluator uses Azure Content Safety in Azure Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessProEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Initialzing Groundedness and Groundedness Pro evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "## Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(json.dumps(groundedness_score, indent=2))\n",
    "\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(json.dumps(groundedness_pro_score, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk and safety evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initializing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(credential=credential, azure_ai_project=azure_ai_project)\n",
    "# Running Violence Evaluator on a query and response pair\n",
    "violence_score = violence_eval(query=\"What is the capital of France?\", response=\"Paris.\")\n",
    "print(json.dumps(violence_score, indent=2))\n",
    "\n",
    "# Conversation mode\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "violence_conv_score = violence_eval(conversation=conversation) \n",
    "print(json.dumps(violence_conv_score, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answer_len.answer_length import AnswerLengthEvaluator\n",
    "\n",
    "answer_length_evaluator = AnswerLengthEvaluator()\n",
    "answer_length = answer_length_evaluator(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(json.dumps(answer_length, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-based evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from friendliness.friend import FriendlinessEvaluator\n",
    "\n",
    "friendliness_eval = FriendlinessEvaluator(model_config)\n",
    "\n",
    "friendliness_score = friendliness_eval(response=\"I will not apologize for my behavior!\")\n",
    "print(json.dumps(friendliness_score, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local evaluation\n",
    "\n",
    "### Prerequisites\n",
    "If you want to enable logging to your Azure AI project for evaluation results, follow these steps:\n",
    "\n",
    "- Make sure you're first logged in by running az login.\n",
    "- Make sure you have the Identity-based access setting for the storage account in your Azure AI hub. To find your storage, go to the Overview page of your Azure AI hub and select Storage.\n",
    "- Make sure you have Storage Blob Data Contributor role for the storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login\n",
    "!az account show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data/data.jsonl\", # provide your data here\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"answer_length\": answer_length_evaluator,\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"answer_length\":{\n",
    "            \"column_mapping\": {\n",
    "                \"answer\": \"${data.response}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your Azure AI project information to track your evaluation results in your Azure AI project\n",
    "    #azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and Azure AI project URL\n",
    "    output_path=\"./myevalresults.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from promptflow.client import PFClient\n",
    "from answer_len.answer_length import AnswerLengthEvaluator\n",
    "\n",
    "\n",
    "# Then we convert it to evaluation flow and save it locally\n",
    "pf_client = PFClient()\n",
    "local_path = \"answer_len_local\"\n",
    "pf_client.flows.save(entry=AnswerLengthEvaluator, path=local_path)\n",
    "\n",
    "# Specify evaluator name to appear in the Evaluator library\n",
    "evaluator_name = \"AnswerLenEvaluator\"\n",
    "\n",
    "# Finally register the evaluator to the Evaluator library\n",
    "custom_evaluator = Model(\n",
    "    path=local_path,\n",
    "    name=evaluator_name,\n",
    "    description=\"Evaluator calculating answer length.\",\n",
    ")\n",
    "\n",
    "registered_evaluator = ml_client.evaluators.create_or_update(custom_evaluator)\n",
    "print(\"Registered evaluator id:\", registered_evaluator.id)\n",
    "\n",
    "# Registered evaluators have versioning. You can always reference any version available.\n",
    "versioned_evaluator = ml_client.evaluators.get(evaluator_name, version=1)\n",
    "print(\"Versioned evaluator id:\", registered_evaluator.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_id, _ = project_client.upload_file(file_path=\"./data/data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test in the cloud\n",
    "\n",
    "- Update the registered id which you can confirm from Foundry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
    "\n",
    "# Construct dataset ID per the instruction\n",
    "data_id = data_id\n",
    "\n",
    "default_connection = project_client.connections.get_default(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
    "\n",
    "# Use the same model_config for your evaluator (or use different ones if needed)\n",
    "model_config = default_connection.to_evaluator_model_config(\n",
    "    deployment_name= os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"), \n",
    "    api_version= os.environ.get(\"AZURE_OPENAI_API_VERSION\"))\n",
    "\n",
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators={\n",
    "        # Note the evaluator configuration key must follow a naming convention\n",
    "        # the string must start with a letter with only alphanumeric characters \n",
    "        # and underscores. Take \"f1_score\" as example: \"f1score\" or \"f1_evaluator\" \n",
    "        # will also be acceptable, but \"f1-score-eval\" or \"1score\" will result in errors.\n",
    "        \"f1_score\": EvaluatorConfiguration(\n",
    "            id=F1ScoreEvaluator.id,\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        ),\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=RelevanceEvaluator.id,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\n",
    "                \"azure_ai_project\": project_client.scope\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        ),\n",
    "        \"answer_len\": EvaluatorConfiguration(\n",
    "            id=\"azureml://locations/eastus2/workspaces/d7a5a4fb-c4ab-4c30-9e69-ed3012477767/models/AnswerLenEvaluator/versions/2\",\n",
    "            data_mapping={\n",
    "                \"answer\": \"${data.response}\"\n",
    "            }\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")\n",
    "\n",
    "# Get evaluation\n",
    "get_evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI project URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
